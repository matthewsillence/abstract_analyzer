{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstract_analyzer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTA9bww11VHhT/tJQZtNsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewsillence/abstract_analyzer/blob/main/abstract_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z07yS4MvEI8S",
        "outputId": "b14d7c56-5fd9-4288-ef56-7542685e7c77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#This script is adapted from: https://andrewpwheeler.com/2016/06/08/sentence-length-in-academic-articles/ \n",
        "#import NLTK\n",
        "import nltk\n",
        "#need to download this for the English sentence tokenizer files\n",
        "\n",
        "#download Punkt. This splits up punctuation\n",
        "nltk.download('punkt')\n",
        "\n",
        "#import PrettyPrint\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Open the file, read contents and tokenize\n",
        "F_IN = 'abstract.txt'\n",
        "\n",
        "with open(F_IN, 'r', encoding='utf-8') as f_in:\n",
        "    raw_text = f_in.read()\n",
        "    word_tok = nltk.word_tokenize(raw_text)\n",
        "\n",
        "#need to take out commas plus other stuff\n",
        "NoWord = [',','(',')',':',';','.','%','\\x96','{','}','[',']','!','?',\"''\",\"``\"]\n",
        "word_tok2 = [i for i in word_tok if i not in NoWord]\n",
        "nw = len(word_tok2)\n",
        "\n",
        "#Print number of tokens\n",
        "print(nw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9tfkXdkLLUR",
        "outputId": "7201a921-4cdf-4af9-d110-9ed44f5ac7f1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of sentences\n",
        "sent_tok = nltk.sent_tokenize(raw_text)\n",
        "ns = len(sent_tok)\n",
        "print(ns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mG8Klk3Et-D",
        "outputId": "fed9ed3d-07d8-4bea-a759-d7084e0b6dad"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Average Sentence length are words divided by sentences\n",
        "print(nw/ns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IklJ0MmWIHar",
        "outputId": "a902da56-67b1-4c6c-d27e-f34ac53f2ec5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23.818181818181817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and tag words\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "pos_tags = nltk.pos_tag(word_tok2)\n",
        "pprint.pprint(pos_tags[0:9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW5_CuFIOJy3",
        "outputId": "612b78ad-f4ca-4e03-ae8e-f4633b6433c7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Shakespeare', 'NN'),\n",
            " ('and', 'CC'),\n",
            " ('the', 'DT'),\n",
            " ('BBC', 'NNP'),\n",
            " ('are', 'VBP'),\n",
            " ('two', 'CD'),\n",
            " ('of', 'IN'),\n",
            " ('the', 'DT'),\n",
            " ('cultural', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pos_tags)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwC4ohNnOlk3",
        "outputId": "9a8219bc-b512-454a-b8dd-eeee47ce1d3f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               0    1\n",
            "0    Shakespeare   NN\n",
            "1            and   CC\n",
            "2            the   DT\n",
            "3            BBC  NNP\n",
            "4            are  VBP\n",
            "..           ...  ...\n",
            "257     listings  NNS\n",
            "258          are  VBP\n",
            "259         held  VBN\n",
            "260           if   IN\n",
            "261        known  VBN\n",
            "\n",
            "[262 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('tags2.csv')"
      ],
      "metadata": {
        "id": "ER3yYc25OoYW"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}